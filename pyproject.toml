[project]
name = "vlm-yt"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "accelerate>=1.12.0",
    "flash-attn ; sys_platform == 'linux'",
    "huggingface-hub>=1.2.3",
    "pandas>=2.3.3",
    "qwen-vl-utils[decord]==0.0.8",
    "requests>=2.32.5",
    "torch>=2.9.1",
    "torchaudio>=2.9.1",
    "torchvision>=0.24.1",
    "transformers",
]

[tool.uv.sources]
transformers = { git = "https://github.com/huggingface/transformers" }
flash-attn = { url = "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.22/flash_attn-2.8.1+cu128torch2.9-cp312-cp312-linux_x86_64.whl" }
